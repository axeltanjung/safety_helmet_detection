{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNsGJiZw6u5dg1IUbU/rb8n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axeltanjung/safety_helmet_detection/blob/main/notebook/safety_helmet_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Dataset"
      ],
      "metadata": {
        "id": "MsHoXPWyyTV3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl4h168-tG3O",
        "outputId": "5a88f9b3-d835-44d0-a448-ee67105f4fe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'hard-hat-detection' dataset.\n",
            "Path to dataset files: /kaggle/input/hard-hat-detection\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"andrewmvd/hard-hat-detection\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /kaggle/input/hard-hat-detection /kaggle/working/"
      ],
      "metadata": {
        "id": "xVY6ZlCSu7de"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics pillow lxml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrJLPo4wudiA",
        "outputId": "4ea5b9af-4e7f-4f72-a89a-15bcaa552f06"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.241-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (6.0.2)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cu126)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.11.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Downloading ultralytics-8.3.241-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.241 ultralytics-thop-2.0.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess Dataset"
      ],
      "metadata": {
        "id": "eRZ3H6NXyWdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image\n",
        "\n",
        "# Paths (Kaggle writable directory). Make sure these folders exist in /kaggle/working/\n",
        "DATASET_DIR = \"/kaggle/working\"\n",
        "IMAGE_DIR = os.path.join(DATASET_DIR, \"images\")\n",
        "ANNOTATION_DIR = os.path.join(DATASET_DIR, \"annotations\")\n",
        "LABEL_DIR = os.path.join(DATASET_DIR, \"labels\")\n",
        "\n",
        "# Classes (edit if needed)\n",
        "CLASSES = [\"helmet\"]  # e.g., [\"helmet\", \"person\"] if you have multiple\n",
        "\n",
        "# Ensure required folders exist\n",
        "for p in [IMAGE_DIR, ANNOTATION_DIR]:\n",
        "    if not os.path.isdir(p):\n",
        "        raise FileNotFoundError(f\"Folder not found: {p}. \"\n",
        "                                f\"If your dataset is under /kaggle/input, copy it first:\\n\"\n",
        "                                f\"!cp -r /kaggle/input/hard-hat-detection /kaggle/working/\\n\"\n",
        "                                f\"And then set IMAGE_DIR='/kaggle/working/hard-hat-detection/images', \"\n",
        "                                f\"ANNOTATION_DIR='/kaggle/working/hard-hat-detection/annotations'\")\n",
        "\n",
        "os.makedirs(LABEL_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def normalize_bbox(img_size, box):\n",
        "    \"\"\"Convert Pascal VOC (xmin, xmax, ymin, ymax) to YOLO (xc, yc, w, h) normalized.\"\"\"\n",
        "    img_w, img_h = img_size\n",
        "    xmin, xmax, ymin, ymax = box\n",
        "\n",
        "    # Clip to image bounds (defensive)\n",
        "    xmin = max(0.0, min(xmin, img_w))\n",
        "    xmax = max(0.0, min(xmax, img_w))\n",
        "    ymin = max(0.0, min(ymin, img_h))\n",
        "    ymax = max(0.0, min(ymax, img_h))\n",
        "\n",
        "    # Compute normalized values\n",
        "    x_center = ((xmin + xmax) / 2.0) / img_w\n",
        "    y_center = ((ymin + ymax) / 2.0) / img_h\n",
        "    width = (xmax - xmin) / img_w\n",
        "    height = (ymax - ymin) / img_h\n",
        "\n",
        "    # Guard against zero/negative boxes\n",
        "    if width <= 0 or height <= 0:\n",
        "        return None\n",
        "\n",
        "    # Guard against out-of-range\n",
        "    x_center = min(max(x_center, 0.0), 1.0)\n",
        "    y_center = min(max(y_center, 0.0), 1.0)\n",
        "    width = min(max(width, 1e-6), 1.0)\n",
        "    height = min(max(height, 1e-6), 1.0)\n",
        "\n",
        "    return x_center, y_center, width, height\n",
        "\n",
        "\n",
        "converted = 0\n",
        "skipped = 0\n",
        "\n",
        "for xml_file in os.listdir(ANNOTATION_DIR):\n",
        "    if not xml_file.lower().endswith(\".xml\"):\n",
        "        continue\n",
        "\n",
        "    xml_path = os.path.join(ANNOTATION_DIR, xml_file)\n",
        "    try:\n",
        "        tree = ET.parse(xml_path)\n",
        "        root = tree.getroot()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to parse XML: {xml_path} ({e})\")\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    # Try to read image filename\n",
        "    node_filename = root.find(\"filename\")\n",
        "    if node_filename is None or not node_filename.text:\n",
        "        print(f\"‚ùå Missing <filename> in: {xml_file}\")\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    image_name = node_filename.text.strip()\n",
        "    image_path = os.path.join(IMAGE_DIR, image_name)\n",
        "\n",
        "    if not os.path.exists(image_path):\n",
        "        print(f\"‚ùå Image not found for XML: {xml_file} ‚Üí {image_path}\")\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    # Get image size (prefer reading actual image)\n",
        "    try:\n",
        "        with Image.open(image_path) as im:\n",
        "            img_w, img_h = im.size\n",
        "    except Exception:\n",
        "        # Fallback: read from XML <size>\n",
        "        size_node = root.find(\"size\")\n",
        "        if size_node is None:\n",
        "            print(f\"‚ùå Missing image size for: {image_name}\")\n",
        "            skipped += 1\n",
        "            continue\n",
        "        img_w = float(size_node.find(\"width\").text)\n",
        "        img_h = float(size_node.find(\"height\").text)\n",
        "\n",
        "    label_file = os.path.splitext(image_name)[0] + \".txt\"\n",
        "    label_path = os.path.join(LABEL_DIR, label_file)\n",
        "\n",
        "    wrote_any = False\n",
        "    with open(label_path, \"w\") as f:\n",
        "        for obj in root.findall(\"object\"):\n",
        "            name_node = obj.find(\"name\")\n",
        "            if name_node is None or not name_node.text:\n",
        "                continue\n",
        "\n",
        "            class_name = name_node.text.strip()\n",
        "            if class_name not in CLASSES:\n",
        "                # Skip unknown classes\n",
        "                continue\n",
        "\n",
        "            class_id = CLASSES.index(class_name)\n",
        "            bndbox = obj.find(\"bndbox\")\n",
        "            if bndbox is None:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                xmin = float(bndbox.find(\"xmin\").text)\n",
        "                ymin = float(bndbox.find(\"ymin\").text)\n",
        "                xmax = float(bndbox.find(\"xmax\").text)\n",
        "                ymax = float(bndbox.find(\"ymax\").text)\n",
        "            except Exception:\n",
        "                # Malformed bndbox\n",
        "                continue\n",
        "\n",
        "            yolo_bbox = normalize_bbox((img_w, img_h), (xmin, xmax, ymin, ymax))\n",
        "            if yolo_bbox is None:\n",
        "                # Invalid box\n",
        "                continue\n",
        "\n",
        "            f.write(\n",
        "                f\"{class_id} \" +\n",
        "                \" \".join(f\"{v:.6f}\" for v in yolo_bbox) +\n",
        "                \"\\n\"\n",
        "            )\n",
        "            wrote_any = True\n",
        "\n",
        "    if wrote_any:\n",
        "        converted += 1\n",
        "    else:\n",
        "        # Remove empty label file to avoid \"no labels\" issues later\n",
        "        try:\n",
        "            os.remove(label_path)\n",
        "        except FileNotFoundError:\n",
        "            pass\n",
        "\n",
        "print(f\"‚úÖ Conversion done. Labels written for {converted} images, skipped {skipped}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iuYr5POudeY",
        "outputId": "a8088431-c131-4804-99f3-fa6bb25d74d3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Conversion done. Labels written for 4581 images, skipped 0.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Train / Test"
      ],
      "metadata": {
        "id": "QUNDybH1yYsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Sesuaikan path dataset kamu\n",
        "BASE = \"/kaggle/working\"\n",
        "IMAGE_DIR = os.path.join(BASE, \"images\")         # contoh: \"/kaggle/working/hard-hat-detection/images\"\n",
        "LABEL_DIR = os.path.join(BASE, \"labels\")         # contoh: \"/kaggle/working/hard-hat-detection/labels\"\n",
        "\n",
        "# Jika dataset kamu ada di /kaggle/working/hard-hat-detection, gunakan:\n",
        "# IMAGE_DIR = \"/kaggle/working/hard-hat-detection/images\"\n",
        "# LABEL_DIR = \"/kaggle/working/hard-hat-detection/labels\"\n",
        "\n",
        "# Pastikan folder ada\n",
        "for p in [IMAGE_DIR, LABEL_DIR]:\n",
        "    if not os.path.isdir(p):\n",
        "        raise FileNotFoundError(f\"Folder tidak ditemukan: {p}. Pastikan sudah copy dari /kaggle/input ke /kaggle/working\")\n",
        "\n",
        "# 1) Buat label kosong untuk semua gambar yang belum ada .txt\n",
        "image_files = [f for f in os.listdir(IMAGE_DIR) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
        "\n",
        "created_empty = 0\n",
        "for img in image_files:\n",
        "    stem = os.path.splitext(img)[0]\n",
        "    lbl = os.path.join(LABEL_DIR, stem + \".txt\")\n",
        "    if not os.path.exists(lbl):\n",
        "        # Buat file label kosong (valid buat YOLOv8)\n",
        "        open(lbl, \"w\").close()\n",
        "        created_empty += 1\n",
        "\n",
        "print(f\"‚úÖ Dibuat {created_empty} file label kosong untuk gambar tanpa anotasi.\")\n",
        "\n",
        "# 2) Siapkan folder split\n",
        "for split in [\"train\", \"val\"]:\n",
        "    os.makedirs(os.path.join(IMAGE_DIR, split), exist_ok=True)\n",
        "    os.makedirs(os.path.join(LABEL_DIR, split), exist_ok=True)\n",
        "\n",
        "# 3) Split train/val\n",
        "TRAIN_RATIO = 0.8\n",
        "random.shuffle(image_files)\n",
        "split_idx = int(len(image_files) * TRAIN_RATIO)\n",
        "train_imgs = image_files[:split_idx]\n",
        "val_imgs = image_files[split_idx:]\n",
        "\n",
        "def move_pair(img_list, split):\n",
        "    moved = 0\n",
        "    for img in img_list:\n",
        "        src_img = os.path.join(IMAGE_DIR, img)\n",
        "        dst_img = os.path.join(IMAGE_DIR, split, img)\n",
        "\n",
        "        stem = os.path.splitext(img)[0]\n",
        "        src_lbl = os.path.join(LABEL_DIR, stem + \".txt\")\n",
        "        dst_lbl = os.path.join(LABEL_DIR, split, stem + \".txt\")\n",
        "\n",
        "        # Pindahkan image\n",
        "        shutil.move(src_img, dst_img)\n",
        "        # Pindahkan label (dijamin ada karena kita buat kosong jika tidak ada)\n",
        "        shutil.move(src_lbl, dst_lbl)\n",
        "        moved += 1\n",
        "    return moved\n",
        "\n",
        "moved_train = move_pair(train_imgs, \"train\")\n",
        "moved_val = move_pair(val_imgs, \"val\")\n",
        "\n",
        "print(f\"‚úÖ Split selesai. Train: {moved_train} | Val: {moved_val}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miwXgSQoudbg",
        "outputId": "c411c548-3d97-432d-c8c1-e676353e31f0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dibuat 419 file label kosong untuk gambar tanpa anotasi.\n",
            "‚úÖ Split selesai. Train: 4000 | Val: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os\n",
        "print(\"Images total:\", len(glob.glob(os.path.join(IMAGE_DIR, \"*.[jp][pn]g\"))))\n",
        "print(\"Labels total:\", len(glob.glob(os.path.join(LABEL_DIR, \"*.txt\"))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ie7wlPKUudYi",
        "outputId": "41e12638-c36a-4122-c654-012ef07f2a5d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images total: 0\n",
            "Labels total: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_yaml = \"\"\"\n",
        "path: /kaggle/working/\n",
        "train: images/train\n",
        "val: images/val\n",
        "\n",
        "names:\n",
        "  0: helmet\n",
        "\"\"\"\n",
        "\n",
        "with open(\"/kaggle/working/data.yaml\", \"w\") as f:\n",
        "    f.write(data_yaml)\n",
        "\n",
        "print(\"‚úÖ data.yaml created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSik6f7kudRb",
        "outputId": "682c3357-fe1f-4f4f-fb96-41ae24b573d6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ data.yaml created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"yolov8s.pt\")\n",
        "\n",
        "model.train(\n",
        "    data=\"/kaggle/working/data.yaml\",\n",
        "    epochs=100,\n",
        "    imgsz=640,\n",
        "    batch=16\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HDcy6DktW-u",
        "outputId": "f6a23359-0d51-4a91-fbbb-7cddafcf7202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 21.5MB 422.7MB/s 0.1s\n",
            "Ultralytics 8.3.241 üöÄ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/kaggle/working/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 755.1KB 3.1MB/s 0.2s\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
            " 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
            "Model summary: 129 layers, 11,135,987 parameters, 11,135,971 gradients, 28.6 GFLOPs\n",
            "\n",
            "Transferred 349/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5.4MB 187.4MB/s 0.0s\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 3856.2¬±1220.3 MB/s, size: 277.1 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/labels/train... 4000 images, 330 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4000/4000 1.2Kit/s 3.3s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/labels/train.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1462.4¬±939.0 MB/s, size: 248.4 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/labels/val... 1000 images, 89 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1000/1000 772.5it/s 1.3s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/labels/val.cache\n",
            "Plotting labels to /content/runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/detect/train\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      1/100       3.7G       1.49      1.284      1.249        113        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 250/250 2.4it/s 1:46\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 32/32 2.1it/s 15.3s\n",
            "                   all       1000       3872      0.854      0.712      0.811      0.445\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      2/100      4.49G      1.443     0.9896       1.22         90        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 250/250 2.3it/s 1:48\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 32/32 2.0it/s 16.2s\n",
            "                   all       1000       3872       0.86      0.746      0.842      0.485\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      3/100      4.53G      1.441     0.9691       1.23         64        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 250/250 2.2it/s 1:51\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 32/32 2.3it/s 13.8s\n",
            "                   all       1000       3872      0.873      0.767      0.857       0.48\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      4/100      4.57G      1.417     0.9428      1.221         66        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 250/250 2.5it/s 1:38\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 32/32 2.2it/s 14.5s\n",
            "                   all       1000       3872      0.868      0.806      0.887      0.509\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      5/100       4.6G      1.394     0.8795        1.2         75        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 250/250 2.5it/s 1:38\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 32/32 2.3it/s 14.0s\n",
            "                   all       1000       3872       0.91       0.81      0.905      0.542\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      6/100      4.64G      1.373     0.8513      1.199         80        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 250/250 2.5it/s 1:39\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 32/32 2.2it/s 14.6s\n",
            "                   all       1000       3872      0.892      0.817      0.897      0.535\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      7/100      4.68G      1.352     0.8185      1.175         81        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 250/250 2.6it/s 1:37\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 32/32 2.2it/s 14.4s\n",
            "                   all       1000       3872      0.906      0.831      0.911      0.563\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      8/100      4.71G      1.344     0.8076      1.177        124        640: 71% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÄ‚îÄ‚îÄ‚îÄ 177/250 2.1it/s 1:11<35.3s"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uji cepat pada folder val / sample image"
      ],
      "metadata": {
        "id": "9EGyRpPm0KV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_path = model.ckpt_path if hasattr(model, \"ckpt_path\") else \"runs/detect/train/weights/best.pt\"\n",
        "best_model = YOLO(best_path)\n",
        "\n",
        "# Prediksi folder val\n",
        "best_model.predict(\n",
        "    source=os.path.join(BASE_DIR, \"images/val\"),\n",
        "    conf=0.4,\n",
        "    save=True,   # simpan visualisasi\n",
        "    project=\"runs/detect\",\n",
        "    name=\"predict_val\",\n",
        "    exist_ok=True\n",
        ")"
      ],
      "metadata": {
        "id": "qGvu861_yJyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NO‚ÄëHelmet Logic (heuristik bagian kepala)\n",
        "\n",
        "Ide: Untuk setiap person box, ambil area kepala sebagai bagian atas dari bounding box (mis. top 35%). Jika tidak ada helmet box yang overlap dengan area kepala (IoU > threshold kecil), maka itu violation (no‚Äëhelmet).\n",
        "\n",
        "Ini heuristik yang bekerja baik untuk kamera statis dari atas/bawah‚Äîkalau kamera miring/occlusion berat, kita bisa refine (mis. deteksi pose atau segmentasi kepala)."
      ],
      "metadata": {
        "id": "Pt6Hmz800ROU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "\n",
        "best_model = YOLO(best_path)\n",
        "\n",
        "# Hyperparameter heuristik:\n",
        "HEAD_RATIO = 0.35    # proporsi tinggi box person untuk area kepala (0.30‚Äì0.40 umum)\n",
        "IOU_THRESH = 0.10     # threshold overlap minimal antara head-region dan helmet box\n",
        "\n",
        "def iou(boxA, boxB):\n",
        "    # box = [x1, y1, x2, y2]\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "    interW = max(0, xB - xA)\n",
        "    interH = max(0, yB - yA)\n",
        "    interArea = interW * interH\n",
        "    if interArea == 0:\n",
        "        return 0.0\n",
        "    areaA = (boxA[2]-boxA[0]) * (boxA[3]-boxA[1])\n",
        "    areaB = (boxB[2]-boxB[0]) * (boxB[3]-boxB[1])\n",
        "    return interArea / (areaA + areaB - interArea + 1e-6)\n",
        "\n",
        "def detect_no_helmet(image_bgr):\n",
        "    # Run inference\n",
        "    res = best_model.predict(image_bgr, imgsz=640, conf=0.4, verbose=False)[0]\n",
        "    boxes = res.boxes\n",
        "    clss = boxes.cls.cpu().numpy().astype(int)\n",
        "    xyxy = boxes.xyxy.cpu().numpy()  # [N, 4]\n",
        "    # 0: person, 1: helmet (sesuai data.yaml)\n",
        "\n",
        "    person_boxes = xyxy[clss == 0]\n",
        "    helmet_boxes = xyxy[clss == 1]\n",
        "\n",
        "    violations = []  # list of indices atau koordinat\n",
        "    for pb in person_boxes:\n",
        "        x1, y1, x2, y2 = pb\n",
        "        h = y2 - y1\n",
        "        head_h = HEAD_RATIO * h\n",
        "        head_box = [x1, y1, x2, y1 + head_h]  # kepala: bagian atas dari person box\n",
        "\n",
        "        has_helmet = False\n",
        "        for hb in helmet_boxes:\n",
        "            if iou(head_box, hb) >= IOU_THRESH:\n",
        "                has_helmet = True\n",
        "                break\n",
        "        if not has_helmet:\n",
        "            violations.append(head_box)\n",
        "\n",
        "    return xyxy, clss, violations\n",
        "\n",
        "# Uji ke 1 image (ganti path ke test image)\n",
        "test_img_path = os.path.join(BASE_DIR, \"images/val\", os.listdir(os.path.join(BASE_DIR, \"images/val\"))[0])\n",
        "img = cv2.imread(test_img_path)\n",
        "\n",
        "xyxy, clss, violations = detect_no_helmet(img)\n",
        "print(f\"Person+Helmet detections: {len(xyxy)} | Violations (no-helmet): {len(violations)}\")"
      ],
      "metadata": {
        "id": "i7Sq1InZ0Xrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Realtime CCTV (RTSP) dengan overlay NO‚Äëhelmet"
      ],
      "metadata": {
        "id": "rcqvKCVD0bgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "\n",
        "RTSP_URL = \"rtsp://user:pass@IP_CAMERA:554/Streaming/Channels/101\"  # Ganti sesuai kamera\n",
        "\n",
        "cap = cv2.VideoCapture(RTSP_URL)\n",
        "if not cap.isOpened():\n",
        "    print(\"‚ùå Tidak bisa membuka stream. Cek RTSP_URL / kredensial.\")\n",
        "\n",
        "# Warna & style\n",
        "GREEN = (0, 200, 0)\n",
        "RED   = (0, 0, 255)\n",
        "YELLOW= (0, 255, 255)\n",
        "\n",
        "def draw_box(img, box, color, label=None):\n",
        "    x1, y1, x2, y2 = map(int, box)\n",
        "    cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
        "    if label:\n",
        "        cv2.putText(img, label, (x1, max(0, y1-10)),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
        "\n",
        "fps_hist = []\n",
        "while True:\n",
        "    ok, frame = cap.read()\n",
        "    if not ok:\n",
        "        print(\"Stream terputus / buffer kosong.\")\n",
        "        time.sleep(0.5)\n",
        "        continue\n",
        "\n",
        "    t0 = time.time()\n",
        "    xyxy, clss, violations = detect_no_helmet(frame)\n",
        "\n",
        "    # Gambar semua deteksi\n",
        "    for box, c in zip(xyxy, clss):\n",
        "        label = \"person\" if c == 0 else \"helmet\"\n",
        "        color = GREEN if c == 1 else YELLOW\n",
        "        draw_box(frame, box, color, label)\n",
        "\n",
        "    # Tampilkan violations (kepala merah)\n",
        "    for hb in violations:\n",
        "        draw_box(frame, hb, RED, \"NO-HELMET\")\n",
        "\n",
        "    # FPS\n",
        "    fps = 1.0 / (time.time() - t0 + 1e-6)\n",
        "    fps_hist.append(fps)\n",
        "    fps_hist = fps_hist[-30:]\n",
        "    cv2.putText(frame, f\"FPS: {np.mean(fps_hist):.1f}\", (10, 30),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
        "\n",
        "    # Tampilkan hasil (di Colab gunakan cv2_imshow; di Jupyter lokal pakai cv2.imshow)\n",
        "    # Colab:\n",
        "    from google.colab.patches import cv2_imshow\n",
        "    cv2_imshow(frame)\n",
        "\n",
        "    # Keluar setelah beberapa frame (untuk demo)\n",
        "    # break\n"
      ],
      "metadata": {
        "id": "XZVVKkXO0dit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Untuk deployment di laptop/PC: ganti cv2_imshow dengan cv2.imshow(\"Helmet Detection\", frame) dan tambahkan if cv2.waitKey(1) & 0xFF == 27: break. Jangan lupa cap.release(); cv2.destroyAllWindows()."
      ],
      "metadata": {
        "id": "6-7ndaAs0iC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export ke ONNX & TensorRT"
      ],
      "metadata": {
        "id": "FhMWY9BB0kTa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ONNX universal; TensorRT memberi latency/throughput terbaik di NVIDIA GPU (butuh runtime TensorRT & CUDA di environment). Kalau TensorRT belum tersedia, gunakan ONNX dulu."
      ],
      "metadata": {
        "id": "WxfZSlDa0ogK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "best_model = YOLO(best_path)\n",
        "\n",
        "# Export ONNX (dynamic batch & opset umum)\n",
        "best_model.export(\n",
        "    format=\"onnx\",\n",
        "    opset=12,\n",
        "    dynamic=True,\n",
        "    imgsz=640\n",
        ")\n",
        "# hasil: runs/detect/train/weights/best.onnx\n",
        "\n",
        "# Export TensorRT (butuh TensorRT terpasang)\n",
        "# Untuk INT8 perlu kalibrasi dataset; di sini contoh FP16 (\"half=True\")\n",
        "try:\n",
        "    best_model.export(\n",
        "        format=\"engine\",\n",
        "        half=True,    # FP16\n",
        "        imgsz=640\n",
        "    )\n",
        "    print(\"‚úÖ TensorRT engine diexport: best.engine\")\n",
        "except Exception as e:\n",
        "    print(\"‚ö†Ô∏è Gagal export TensorRT. Pakai ONNX dulu. Error:\", e)\n"
      ],
      "metadata": {
        "id": "ZHJhxlhZ0mJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference dengan ONNX Runtime"
      ],
      "metadata": {
        "id": "oday9wj80sAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kalau kamu ingin menjalankan model ONNX tanpa ultralytics, bisa pakai onnxruntime:"
      ],
      "metadata": {
        "id": "DwMCZa3p0ujr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install onnxruntime-gpu\n",
        "\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "onnx_path = \"runs/detect/train/weights/best.onnx\"\n",
        "session = ort.InferenceSession(onnx_path, providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
        "\n",
        "def preprocess(img_bgr, size=640):\n",
        "    img = cv2.resize(img_bgr, (size, size))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = img.astype(np.float32) / 255.0\n",
        "    img = img.transpose(2, 0, 1)[None]  # NCHW\n",
        "    return img\n",
        "\n",
        "# Sesuaikan input/output names sesuai model ONNX (cek session.get_inputs()/get_outputs())\n",
        "inp_name = session.get_inputs()[0].name\n",
        "out_names = [o.name for o in session.get_outputs()]\n",
        "\n",
        "img = cv2.imread(test_img_path)\n",
        "inp = preprocess(img)\n",
        "outputs = session.run(out_names, {inp_name: inp})\n",
        "print(\"ONNX outputs:\", [o.shape for o in outputs])\n"
      ],
      "metadata": {
        "id": "3d6GZDjO0svn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}